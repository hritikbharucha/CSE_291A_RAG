{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadb2a85-bef8-45a5-bc76-9fe345ed3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers accelerate bitsandbytes torch pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502b5310-1aad-43e0-84b8-227a18d18c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca96197-9575-4a66-a890-4c32d5e67805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "ROOT = \"/Users/zedsiyed/Downloads/CSE_291A_RAG/question_generation/\"\n",
    "DATA_PATH = ROOT + \"data.csv\"  # Kaggle Global News Dataset\n",
    "OUTPUT_PATH = ROOT + \"sample_requests.jsonl\"\n",
    "N_SAMPLES = 10\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.dropna(subset=[\"content\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cacd388-4376-46b0-9dd9-19562f22c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df.sample(N_SAMPLES, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f515c61-659c-458e-accd-8d197a4c73b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model mistralai/Mistral-7B-Instruct-v0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758a83208bfc44b28da9f7b8e32688dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "# Load Local Model\n",
    "print(f\"Loading model {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=120,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131056ad-fca5-4324-9166-c07f0b415569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "PROMPT_TEMPLATE = \"\"\"You are creating a dataset for a retrieval-augmented generation (RAG) system.\n",
    "\n",
    "Given this news article:\n",
    "<context>\n",
    "{article}\n",
    "</context>\n",
    "\n",
    "Write one realistic, specific question a reader might ask about this article.\n",
    "The question must require retrieving information from the text (names, dates, causes, outcomes),\n",
    "not just a keyword or title lookup.\n",
    "\n",
    "Output ONLY the question text, nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55254eb9-7b75-47d3-9890-def982e4b4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                  | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|████████████████▋                                                                                                                                                      | 1/10 [10:33<1:34:59, 633.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|█████████████████████████████████▍                                                                                                                                     | 2/10 [20:03<1:19:30, 596.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|██████████████████████████████████████████████████▋                                                                                                                      | 3/10 [26:57<59:51, 513.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|███████████████████████████████████████████████████████████████████▌                                                                                                     | 4/10 [33:54<47:29, 474.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|████████████████████████████████████████████████████████████████████████████████████▌                                                                                    | 5/10 [44:20<44:06, 529.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                   | 6/10 [54:10<36:40, 550.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                  | 7/10 [1:01:30<25:42, 514.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 8/10 [1:09:02<16:28, 494.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                | 9/10 [1:18:25<08:35, 515.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [1:25:30<00:00, 513.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# === 5. Generate Question for Each Article ===\n",
    "pairs = []\n",
    "\n",
    "for _, row in tqdm(samples.iterrows(), total=len(samples)):\n",
    "    article = row[\"content\"].strip()\n",
    "    truncated = article[:1500]\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(article=truncated)\n",
    "    outputs = generator(prompt, do_sample=True, num_return_sequences=1)\n",
    "    question = outputs[0][\"generated_text\"].split(\"</context>\")[-1].strip()\n",
    "\n",
    "    pairs.append({\n",
    "        \"x_i\": question,\n",
    "        \"y_i\": truncated,\n",
    "        \"metadata\": {\n",
    "            \"title\": row.get(\"title\", \"\"),\n",
    "            \"category\": row.get(\"category\", \"\"),\n",
    "            \"published\": row.get(\"published\", \"\")\n",
    "        }\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63c0abc1-41aa-4b69-b0bc-8ef90f3444e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 10 labeled samples to /Users/zedsiyed/Downloads/RAG_project/sample_requests.jsonl\n"
     ]
    }
   ],
   "source": [
    "# === 6. Save to JSONL ===\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in pairs:\n",
    "        json.dump(p, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Saved {len(pairs)} labeled samples to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46de199d-2b0d-402d-a8e9-5e68635820d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_i': 'Write one realistic, specific question a reader might ask about this article.\\nThe question must require retrieving information from the text (names, dates, causes, outcomes),\\nnot just a keyword or title lookup.\\n\\nOutput ONLY the question text, nothing else.\\n\\nWhich scientific competition did Heman Bekele win as a 14-year-old ninth-grader, and what was his innovation that earned him the top spot?', 'y_i': 'A 14-year-old boy has been named \"America\\'s top young scientist\" after developing a bar of soap that could help treat melanoma.\\xa0\\nHeman Bekele, a ninth-grader from Virginia, won the 3M Young Scientis… [+2055 chars]', 'metadata': {'title': 'Young teen wins top science prize for soap that can treat skin cancer', 'category': 'Ethiopia', 'published': ''}}\n"
     ]
    }
   ],
   "source": [
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73628526-a2af-4005-bad7-7a4a640566ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
